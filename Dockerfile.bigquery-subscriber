# Multi-stage build for RustMQ BigQuery Subscriber
# Build stage
FROM rust:1.70-slim-bullseye AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    ca-certificates \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /usr/src/rustmq

# Copy dependency files first for better layer caching
COPY Cargo.toml Cargo.lock ./

# Create a dummy src/main.rs to build dependencies
RUN mkdir src && echo "fn main() {}" > src/main.rs

# Build dependencies (this layer will be cached unless Cargo.toml changes)
RUN cargo build --release --bin rustmq-bigquery-subscriber
RUN rm src/main.rs

# Copy the actual source code
COPY src/ ./src/
COPY .cargo/ ./.cargo/

# Build the BigQuery subscriber binary with optimizations
RUN cargo build --release --bin rustmq-bigquery-subscriber

# Verify the binary was built
RUN ls -la target/release/ && \
    file target/release/rustmq-bigquery-subscriber && \
    ldd target/release/rustmq-bigquery-subscriber

# Runtime stage
FROM debian:bullseye-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    libssl1.1 \
    && rm -rf /var/lib/apt/lists/*

# Create rustmq user and group
RUN groupadd -r rustmq && useradd -r -g rustmq rustmq

# Create necessary directories
RUN mkdir -p /etc/rustmq \
    /var/log/rustmq \
    /var/lib/rustmq \
    /home/rustmq/.config/gcloud

# Set ownership
RUN chown -R rustmq:rustmq /var/log/rustmq /var/lib/rustmq /home/rustmq

# Copy the binary from builder stage
COPY --from=builder /usr/src/rustmq/target/release/rustmq-bigquery-subscriber /usr/local/bin/rustmq-bigquery-subscriber

# Make binary executable
RUN chmod +x /usr/local/bin/rustmq-bigquery-subscriber

# Create default configuration template
RUN cat > /etc/rustmq/bigquery-subscriber.toml << 'EOF'
# BigQuery Subscriber Configuration

# Google Cloud Project Configuration
project_id = "${GCP_PROJECT_ID}"
dataset = "${BIGQUERY_DATASET}"
table = "${BIGQUERY_TABLE}"

[write_method.streaming_inserts]
skip_invalid_rows = false
ignore_unknown_values = false
template_suffix = "${BIGQUERY_TEMPLATE_SUFFIX:-}"

[subscription]
broker_endpoints = ["${RUSTMQ_BROKERS:-localhost:9092}"]
topic = "${RUSTMQ_TOPIC}"
consumer_group = "${CONSUMER_GROUP:-bigquery-subscriber}"
start_offset = "${START_OFFSET:-latest}"
max_messages_per_fetch = 1000
fetch_timeout_ms = 5000
session_timeout_ms = 30000

[auth]
method = "${AUTH_METHOD:-application_default}"
service_account_key_file = "${GOOGLE_APPLICATION_CREDENTIALS:-}"
scopes = [
    "https://www.googleapis.com/auth/bigquery",
    "https://www.googleapis.com/auth/bigquery.insertdata"
]

[batching]
max_rows_per_batch = ${MAX_ROWS_PER_BATCH:-1000}
max_batch_size_bytes = ${MAX_BATCH_SIZE_BYTES:-10485760}
max_batch_latency_ms = ${MAX_BATCH_LATENCY_MS:-1000}
max_concurrent_batches = ${MAX_CONCURRENT_BATCHES:-10}

[schema]
mapping = "${SCHEMA_MAPPING:-direct}"
auto_create_table = ${AUTO_CREATE_TABLE:-false}

[error_handling]
max_retries = ${MAX_RETRIES:-3}
dead_letter_action = "${DEAD_LETTER_ACTION:-log}"

[error_handling.retry_backoff.exponential]
base_ms = ${RETRY_BASE_MS:-1000}
max_ms = ${RETRY_MAX_MS:-30000}

[monitoring]
enable_metrics = ${ENABLE_METRICS:-true}
metrics_prefix = "${METRICS_PREFIX:-rustmq_bigquery_subscriber}"
enable_health_check = ${ENABLE_HEALTH_CHECK:-true}
health_check_addr = "${HEALTH_CHECK_ADDR:-0.0.0.0:8080}"
log_successful_inserts = ${LOG_SUCCESSFUL_INSERTS:-false}
log_level = "${LOG_LEVEL:-info}"
EOF

# Create sample configurations for different scenarios
RUN mkdir -p /etc/rustmq/examples

# Example 1: Basic streaming inserts configuration
RUN cat > /etc/rustmq/examples/basic-streaming.toml << 'EOF'
project_id = "my-gcp-project"
dataset = "analytics"
table = "events"

[write_method.streaming_inserts]
skip_invalid_rows = false
ignore_unknown_values = false

[subscription]
broker_endpoints = ["rustmq-broker-1:9092", "rustmq-broker-2:9092"]
topic = "user-events"
consumer_group = "bigquery-analytics"
start_offset = "latest"

[auth]
method = "application_default"

[schema]
mapping = "direct"
auto_create_table = false
EOF

# Example 2: Custom schema mapping configuration
RUN cat > /etc/rustmq/examples/custom-mapping.toml << 'EOF'
project_id = "my-gcp-project"
dataset = "transformed_data"
table = "processed_events"

[write_method.streaming_inserts]
skip_invalid_rows = true
ignore_unknown_values = true

[subscription]
broker_endpoints = ["rustmq-broker:9092"]
topic = "raw-events"
consumer_group = "bigquery-transformer"

[auth]
method = "service_account"
service_account_key_file = "/etc/gcp/service-account.json"

[schema]
mapping = "custom"

[schema.column_mappings]
"event_id" = "id"
"event_timestamp" = "timestamp"
"user_data.user_id" = "user_id"
"event_data.action" = "action"

[schema.default_values]
"processed_at" = "CURRENT_TIMESTAMP()"
"version" = "1.0"

[batching]
max_rows_per_batch = 500
max_batch_latency_ms = 2000
EOF

# Example 3: High-throughput configuration
RUN cat > /etc/rustmq/examples/high-throughput.toml << 'EOF'
project_id = "my-gcp-project"
dataset = "high_volume"
table = "telemetry"

[write_method.streaming_inserts]
skip_invalid_rows = true
ignore_unknown_values = true

[subscription]
broker_endpoints = ["rustmq-broker-1:9092", "rustmq-broker-2:9092", "rustmq-broker-3:9092"]
topic = "telemetry-data"
consumer_group = "bigquery-telemetry"
max_messages_per_fetch = 5000

[batching]
max_rows_per_batch = 5000
max_batch_size_bytes = 52428800  # 50MB
max_batch_latency_ms = 500
max_concurrent_batches = 50

[error_handling]
max_retries = 5

[error_handling.retry_backoff.exponential]
base_ms = 500
max_ms = 60000

[monitoring]
log_successful_inserts = false
log_level = "warn"
EOF

# Create entrypoint script
RUN cat > /usr/local/bin/entrypoint.sh << 'EOF'
#!/bin/bash
set -e

# Function to substitute environment variables in config file
substitute_env_vars() {
    local file=$1
    # Use envsubst to replace environment variables in the config file
    envsubst < "$file" > "$file.tmp" && mv "$file.tmp" "$file"
}

# Check for required environment variables
if [[ -z "$GCP_PROJECT_ID" ]]; then
    echo "Error: GCP_PROJECT_ID environment variable is required"
    exit 1
fi

if [[ -z "$BIGQUERY_DATASET" ]]; then
    echo "Error: BIGQUERY_DATASET environment variable is required"
    exit 1
fi

if [[ -z "$BIGQUERY_TABLE" ]]; then
    echo "Error: BIGQUERY_TABLE environment variable is required"
    exit 1
fi

if [[ -z "$RUSTMQ_TOPIC" ]]; then
    echo "Error: RUSTMQ_TOPIC environment variable is required"
    exit 1
fi

# Set default values for optional environment variables
export RUSTMQ_BROKERS="${RUSTMQ_BROKERS:-localhost:9092}"
export CONSUMER_GROUP="${CONSUMER_GROUP:-bigquery-subscriber}"
export START_OFFSET="${START_OFFSET:-latest}"
export AUTH_METHOD="${AUTH_METHOD:-application_default}"
export SCHEMA_MAPPING="${SCHEMA_MAPPING:-direct}"
export AUTO_CREATE_TABLE="${AUTO_CREATE_TABLE:-false}"
export MAX_ROWS_PER_BATCH="${MAX_ROWS_PER_BATCH:-1000}"
export MAX_BATCH_SIZE_BYTES="${MAX_BATCH_SIZE_BYTES:-10485760}"
export MAX_BATCH_LATENCY_MS="${MAX_BATCH_LATENCY_MS:-1000}"
export MAX_CONCURRENT_BATCHES="${MAX_CONCURRENT_BATCHES:-10}"
export MAX_RETRIES="${MAX_RETRIES:-3}"
export DEAD_LETTER_ACTION="${DEAD_LETTER_ACTION:-log}"
export RETRY_BASE_MS="${RETRY_BASE_MS:-1000}"
export RETRY_MAX_MS="${RETRY_MAX_MS:-30000}"
export ENABLE_METRICS="${ENABLE_METRICS:-true}"
export METRICS_PREFIX="${METRICS_PREFIX:-rustmq_bigquery_subscriber}"
export ENABLE_HEALTH_CHECK="${ENABLE_HEALTH_CHECK:-true}"
export HEALTH_CHECK_ADDR="${HEALTH_CHECK_ADDR:-0.0.0.0:8080}"
export LOG_SUCCESSFUL_INSERTS="${LOG_SUCCESSFUL_INSERTS:-false}"
export LOG_LEVEL="${LOG_LEVEL:-info}"

# Use custom config file if provided, otherwise use default template
CONFIG_FILE="${CONFIG_FILE:-/etc/rustmq/bigquery-subscriber.toml}"

if [[ ! -f "$CONFIG_FILE" ]]; then
    echo "Config file not found: $CONFIG_FILE"
    echo "Available example configurations:"
    ls -la /etc/rustmq/examples/
    exit 1
fi

# Substitute environment variables in configuration
substitute_env_vars "$CONFIG_FILE"

echo "Starting BigQuery Subscriber with configuration:"
echo "  Project: $GCP_PROJECT_ID"
echo "  Dataset: $BIGQUERY_DATASET"
echo "  Table: $BIGQUERY_TABLE"
echo "  Topic: $RUSTMQ_TOPIC"
echo "  Brokers: $RUSTMQ_BROKERS"
echo "  Config: $CONFIG_FILE"

# Set up Google Cloud authentication if service account key is provided
if [[ -n "$GOOGLE_APPLICATION_CREDENTIALS" && -f "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
    echo "Using service account authentication from: $GOOGLE_APPLICATION_CREDENTIALS"
    export GOOGLE_APPLICATION_CREDENTIALS="$GOOGLE_APPLICATION_CREDENTIALS"
fi

# Switch to rustmq user and execute the subscriber
exec gosu rustmq "$@"
EOF

# Install gosu for proper user switching
RUN curl -o /usr/local/bin/gosu -SL "https://github.com/tianon/gosu/releases/download/1.14/gosu-$(dpkg --print-architecture)" \
    && chmod +x /usr/local/bin/gosu \
    && gosu nobody true

# Install envsubst for config template processing
RUN apt-get update && apt-get install -y gettext-base && rm -rf /var/lib/apt/lists/*

# Make entrypoint executable
RUN chmod +x /usr/local/bin/entrypoint.sh

# Expose health check port
EXPOSE 8080

# Set working directory
WORKDIR /var/lib/rustmq

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Use entrypoint script
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]

# Default command
CMD ["rustmq-bigquery-subscriber", "--config", "/etc/rustmq/bigquery-subscriber.toml"]

# Metadata labels
LABEL maintainer="RustMQ Team <team@rustmq.dev>"
LABEL version="0.1.0"
LABEL description="RustMQ BigQuery Subscriber - Stream RustMQ messages to Google BigQuery"
LABEL org.opencontainers.image.source="https://github.com/rustmq/rustmq"
LABEL org.opencontainers.image.documentation="https://docs.rustmq.dev"
LABEL org.opencontainers.image.vendor="RustMQ"
LABEL org.opencontainers.image.title="RustMQ BigQuery Subscriber"