use rustmq_client::{
    config::{ClientConfig, ConsumerConfig, StartPosition},
    consumer::{ConsumerBuilder, OffsetTracker},
    error::{ClientError, Result},
    message::MessageBuilder,
};
use std::collections::HashMap;
use std::time::Duration;
use tokio::time::Instant;

/// Integration tests for multi-partition Consumer functionality
/// These tests verify the complete end-to-end multi-partition behavior

#[tokio::test]
async fn test_multi_partition_offset_tracker_integration() {
    // Test comprehensive multi-partition offset tracking with realistic scenarios
    let mut tracker = OffsetTracker::default();
    
    // Simulate a realistic multi-partition message consumption scenario
    // 3 partitions with different message arrival patterns
    
    // Partition 0: Regular sequential messages
    for offset in 1..=5 {
        tracker.add_pending_offset(0, offset);
    }
    
    // Partition 1: Messages with gaps (simulating network issues)
    tracker.add_pending_offset(1, 1);
    tracker.add_pending_offset(1, 3); // Gap at offset 2
    tracker.add_pending_offset(1, 4);
    tracker.add_pending_offset(1, 2); // Out of order arrival
    
    // Partition 2: Single message
    tracker.add_pending_offset(2, 1);
    
    // Verify initial state
    assert_eq!(tracker.get_committed_offset(0), 0);
    assert_eq!(tracker.get_committed_offset(1), 0);
    assert_eq!(tracker.get_committed_offset(2), 0);
    
    // Acknowledge messages in various orders to test consecutive tracking
    
    // Partition 0: Acknowledge in order
    for offset in 1..=3 {
        tracker.remove_pending_offset(0, offset);
        tracker.update_consecutive_committed(0);
        assert_eq!(tracker.get_committed_offset(0), offset);
    }
    
    // Partition 1: Acknowledge out of order
    tracker.remove_pending_offset(1, 1);
    tracker.update_consecutive_committed(1);
    assert_eq!(tracker.get_committed_offset(1), 1);
    
    // Acknowledge offset 3 but gap at 2 should prevent advancement
    tracker.remove_pending_offset(1, 3);
    tracker.update_consecutive_committed(1);
    assert_eq!(tracker.get_committed_offset(1), 1); // Still at 1 due to gap
    
    // Fill the gap
    tracker.remove_pending_offset(1, 2);
    tracker.update_consecutive_committed(1);
    assert_eq!(tracker.get_committed_offset(1), 3); // Now advances to 3
    
    // Acknowledge remaining message
    tracker.remove_pending_offset(1, 4);
    tracker.update_consecutive_committed(1);
    assert_eq!(tracker.get_committed_offset(1), 4);
    
    // Partition 2: Simple acknowledge
    tracker.remove_pending_offset(2, 1);
    tracker.update_consecutive_committed(2);
    assert_eq!(tracker.get_committed_offset(2), 1);
    
    // Test commit offset generation
    let commit_offsets = tracker.get_all_committed_offsets();
    assert_eq!(commit_offsets.get(&0), Some(&3));
    assert_eq!(commit_offsets.get(&1), Some(&4));
    assert_eq!(commit_offsets.get(&2), Some(&1));
    
    // Test seeking behavior
    tracker.clear_pending_offsets(0);
    tracker.set_committed_offset(0, 100);
    assert_eq!(tracker.get_committed_offset(0), 100);
    
    // Verify other partitions unaffected by seek
    assert_eq!(tracker.get_committed_offset(1), 4);
    assert_eq!(tracker.get_committed_offset(2), 1);
}

#[tokio::test]
async fn test_consumer_config_for_multi_partition() {
    // Test consumer configuration suitable for multi-partition scenarios
    let consumer_config = ConsumerConfig {
        consumer_id: Some("multi-partition-consumer".to_string()),
        consumer_group: "test-group".to_string(),
        
        // Multi-partition optimized settings
        enable_auto_commit: false, // Manual commits for better control
        auto_commit_interval: Duration::from_secs(30),
        
        // Larger fetch settings for multi-partition efficiency
        fetch_size: 500,
        fetch_timeout: Duration::from_secs(2),
        
        // Start from earliest to ensure we don't miss messages
        start_position: StartPosition::Earliest,
        
        // Conservative retry settings for reliability
        max_retry_attempts: 5,
        dead_letter_queue: Some("failed-multi-partition-messages".to_string()),
    };
    
    // Verify configuration values
    assert_eq!(consumer_config.consumer_id, Some("multi-partition-consumer".to_string()));
    assert_eq!(consumer_config.consumer_group, "test-group");
    assert!(!consumer_config.enable_auto_commit);
    assert_eq!(consumer_config.fetch_size, 500);
    assert_eq!(consumer_config.max_retry_attempts, 5);
    assert!(matches!(consumer_config.start_position, StartPosition::Earliest));
}

#[tokio::test]
async fn test_partition_assignment_simulation() {
    // Simulate partition assignment and state management
    use rustmq_client::consumer::{PartitionAssignment, PartitionInfo};
    
    // Create a partition assignment
    let assignment = PartitionAssignment {
        partitions: vec![0, 1, 2, 4, 7], // Non-consecutive partitions
        assignment_id: "test-assignment-123".to_string(),
        assigned_at: Instant::now(),
    };
    
    // Verify assignment properties
    assert_eq!(assignment.partitions.len(), 5);
    assert!(assignment.partitions.contains(&0));
    assert!(assignment.partitions.contains(&7));
    assert!(!assignment.partitions.contains(&3)); // Gap in assignment
    assert!(!assignment.partitions.contains(&5)); // Gap in assignment
    
    // Test partition info structure
    let partition_info = PartitionInfo {
        partition_id: 0,
        leader_broker: "broker-1.cluster.local:9092".to_string(),
        replica_brokers: vec![
            "broker-1.cluster.local:9092".to_string(),
            "broker-2.cluster.local:9092".to_string(),
            "broker-3.cluster.local:9092".to_string(),
        ],
        high_watermark: 12345,
        low_watermark: 100,
    };
    
    assert_eq!(partition_info.partition_id, 0);
    assert_eq!(partition_info.replica_brokers.len(), 3);
    assert!(partition_info.high_watermark > partition_info.low_watermark);
}

#[tokio::test]
async fn test_multi_partition_seek_scenarios() {
    // Test various seeking scenarios that would occur in production
    let mut tracker = OffsetTracker::default();
    
    // Set up initial committed offsets for multiple partitions
    tracker.set_committed_offset(0, 100);
    tracker.set_committed_offset(1, 250);
    tracker.set_committed_offset(2, 75);
    
    // Add some pending messages
    tracker.add_pending_offset(0, 101);
    tracker.add_pending_offset(0, 102);
    tracker.add_pending_offset(1, 251);
    tracker.add_pending_offset(2, 76);
    
    // Scenario 1: Seek individual partition backward
    tracker.clear_pending_offsets(0);
    tracker.set_committed_offset(0, 50);
    assert_eq!(tracker.get_committed_offset(0), 50);
    
    // Verify other partitions unaffected
    assert_eq!(tracker.get_committed_offset(1), 250);
    assert_eq!(tracker.get_committed_offset(2), 75);
    
    // Scenario 2: Seek individual partition forward
    tracker.set_committed_offset(1, 300);
    assert_eq!(tracker.get_committed_offset(1), 300);
    
    // Scenario 3: Seek to specific timestamp (simulated)
    let current_time = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();
    
    // In a real implementation, this would involve timestamp-to-offset resolution
    // For testing, we simulate the result
    let timestamp_offsets = HashMap::from([
        (0, 1000u64),
        (1, 1500u64),
        (2, 800u64),
    ]);
    
    // Apply timestamp-based seeking
    for (partition, offset) in timestamp_offsets {
        tracker.clear_pending_offsets(partition);
        tracker.set_committed_offset(partition, offset);
    }
    
    assert_eq!(tracker.get_committed_offset(0), 1000);
    assert_eq!(tracker.get_committed_offset(1), 1500);
    assert_eq!(tracker.get_committed_offset(2), 800);
}

#[tokio::test]
async fn test_consumer_lag_calculation() {
    // Test consumer lag calculation across multiple partitions
    let mut tracker = OffsetTracker::default();
    
    // Set up scenario with different lag per partition
    tracker.set_committed_offset(0, 100);
    tracker.set_committed_offset(1, 250);
    tracker.set_committed_offset(2, 75);
    
    // Simulate broker committed offsets (what the broker thinks is committed)
    let broker_offsets = HashMap::from([
        (0, 120u64), // Lag of 20
        (1, 250u64), // No lag
        (2, 100u64), // Lag of 25
    ]);
    
    // Calculate lag per partition
    let mut lag_map = HashMap::new();
    for (partition, broker_offset) in broker_offsets {
        let local_offset = tracker.get_committed_offset(partition);
        let lag = if broker_offset > local_offset {
            broker_offset - local_offset
        } else {
            0
        };
        lag_map.insert(partition, lag);
    }
    
    assert_eq!(lag_map.get(&0), Some(&20));
    assert_eq!(lag_map.get(&1), Some(&0));
    assert_eq!(lag_map.get(&2), Some(&25));
    
    // Test total lag calculation
    let total_lag: u64 = lag_map.values().sum();
    assert_eq!(total_lag, 45);
}

#[tokio::test]
async fn test_partition_pause_resume_simulation() {
    // Test partition pause/resume logic
    let mut paused_partitions = Vec::new();
    let assigned_partitions = vec![0, 1, 2, 3, 4];
    
    // Simulate pausing specific partitions
    let partitions_to_pause = vec![1, 3];
    for partition in &partitions_to_pause {
        if !paused_partitions.contains(partition) {
            paused_partitions.push(*partition);
        }
    }
    
    assert_eq!(paused_partitions.len(), 2);
    assert!(paused_partitions.contains(&1));
    assert!(paused_partitions.contains(&3));
    
    // Test fetching only from active partitions
    let active_partitions: Vec<u32> = assigned_partitions
        .iter()
        .filter(|&p| !paused_partitions.contains(p))
        .copied()
        .collect();
    
    assert_eq!(active_partitions, vec![0, 2, 4]);
    
    // Simulate resuming partitions
    let partitions_to_resume = vec![1];
    for partition in partitions_to_resume {
        paused_partitions.retain(|&p| p != partition);
    }
    
    assert_eq!(paused_partitions, vec![3]);
    
    // Verify final active partitions
    let final_active: Vec<u32> = assigned_partitions
        .iter()
        .filter(|&p| !paused_partitions.contains(p))
        .copied()
        .collect();
    
    assert_eq!(final_active, vec![0, 1, 2, 4]);
}

#[tokio::test]
async fn test_error_handling_per_partition() {
    // Test error handling scenarios specific to multi-partition consumption
    use rustmq_client::error::ClientError;
    
    // Simulate partition-specific errors
    let mut partition_errors = HashMap::new();
    
    // Partition 0: No error
    partition_errors.insert(0, None);
    
    // Partition 1: Offset out of range
    partition_errors.insert(1, Some(ClientError::OffsetOutOfRange { offset: 12345 }));
    
    // Partition 2: Partition not found
    partition_errors.insert(2, Some(ClientError::PartitionNotFound { 
        topic: "test-topic".to_string(), 
        partition: 2 
    }));
    
    // Test error categorization
    for (partition, error) in &partition_errors {
        match error {
            None => {
                // Partition is healthy, can continue processing
                assert_eq!(*partition, 0);
            }
            Some(ClientError::OffsetOutOfRange { .. }) => {
                // Should trigger seeking to latest available offset
                assert_eq!(*partition, 1);
            }
            Some(ClientError::PartitionNotFound { .. }) => {
                // Should pause this partition and request rebalance
                assert_eq!(*partition, 2);
            }
            Some(_) => {
                // Other errors might be retryable
            }
        }
    }
    
    // Test error recovery strategies
    let retryable_partitions: Vec<u32> = partition_errors
        .iter()
        .filter_map(|(partition, error)| {
            match error {
                Some(e) if e.is_retryable() => Some(*partition),
                _ => None,
            }
        })
        .collect();
    
    // Offset out of range is not retryable, but connection errors would be
    assert!(retryable_partitions.is_empty());
}

#[tokio::test]
async fn test_message_ordering_guarantees() {
    // Test that per-partition message ordering is maintained
    let mut tracker = OffsetTracker::default();
    
    // Simulate messages arriving in various orders
    let messages = vec![
        (0, 1), (0, 2), (0, 3),  // Partition 0: in order
        (1, 2), (1, 1), (1, 3),  // Partition 1: out of order
        (2, 1),                  // Partition 2: single message
    ];
    
    // Add all messages as pending
    for (partition, offset) in &messages {
        tracker.add_pending_offset(*partition, *offset);
    }
    
    // Process messages in the order they arrived (which may be out of order per partition)
    for (partition, offset) in &messages {
        tracker.remove_pending_offset(*partition, *offset);
        tracker.update_consecutive_committed(*partition);
    }
    
    // Despite out-of-order arrival, committed offsets should reflect proper ordering
    assert_eq!(tracker.get_committed_offset(0), 3); // All messages processed
    assert_eq!(tracker.get_committed_offset(1), 3); // All messages processed despite out-of-order
    assert_eq!(tracker.get_committed_offset(2), 1); // Single message processed
}

#[tokio::test]
async fn test_consumer_metrics_per_partition() {
    // Test metrics collection for multi-partition scenarios
    use rustmq_client::consumer::ConsumerMetrics;
    use std::sync::atomic::Ordering;
    
    let metrics = ConsumerMetrics::default();
    
    // Simulate processing messages from different partitions
    let partition_message_counts = HashMap::from([
        (0, 100u64),
        (1, 150u64),
        (2, 75u64),
    ]);
    
    let mut total_processed = 0u64;
    for (partition, count) in partition_message_counts {
        // In a real implementation, these would be updated per message
        total_processed += count;
    }
    
    metrics.messages_processed.store(total_processed, Ordering::Relaxed);
    metrics.messages_received.store(total_processed + 10, Ordering::Relaxed); // 10 pending
    
    assert_eq!(metrics.messages_processed.load(Ordering::Relaxed), 325);
    assert_eq!(metrics.messages_received.load(Ordering::Relaxed), 335);
    
    // Calculate processing efficiency
    let processed = metrics.messages_processed.load(Ordering::Relaxed);
    let received = metrics.messages_received.load(Ordering::Relaxed);
    let efficiency = if received > 0 {
        (processed as f64 / received as f64) * 100.0
    } else {
        0.0
    };
    
    assert!((efficiency - 97.0).abs() < 0.1); // ~97% efficiency
}

#[tokio::test]
async fn test_consumer_builder_multi_partition_config() {
    // Test consumer builder with multi-partition optimized configuration
    
    // This would normally require a real client, but we test the config validation
    let multi_partition_config = ConsumerConfig {
        consumer_id: Some("mp-consumer-1".to_string()),
        consumer_group: "multi-partition-group".to_string(),
        
        // Optimized for multi-partition workloads
        enable_auto_commit: false,
        auto_commit_interval: Duration::from_secs(10),
        fetch_size: 1000, // Larger batches for efficiency
        fetch_timeout: Duration::from_millis(500), // Faster polling
        start_position: StartPosition::Latest,
        max_retry_attempts: 3,
        dead_letter_queue: Some("mp-dlq".to_string()),
    };
    
    // Validate configuration is suitable for multi-partition scenarios
    assert!(!multi_partition_config.enable_auto_commit); // Manual commits for precision
    assert!(multi_partition_config.fetch_size >= 500); // Efficient batch sizes
    assert!(multi_partition_config.fetch_timeout <= Duration::from_secs(1)); // Responsive polling
    assert!(multi_partition_config.dead_letter_queue.is_some()); // Error handling
    
    // Test that builder pattern would work (without actually building)
    let builder_config_valid = ConsumerBuilder::new()
        .topic("multi-partition-topic")
        .consumer_group("test-group")
        .config(multi_partition_config);
    
    // If we had a client, this would be: .client(client).build().await
    // For now, we just verify the builder accepts our config
    assert!(true); // Builder construction succeeded
}