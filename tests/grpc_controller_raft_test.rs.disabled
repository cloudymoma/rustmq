//! Comprehensive tests for ControllerRaftService gRPC implementation
//! 
//! This test suite covers all 8 RPC methods of the ControllerRaftService:
//! 1. RequestVote - Raft leader election protocol
//! 2. AppendEntries - Raft log replication and heartbeats
//! 3. InstallSnapshot - Fast follower catch-up mechanism
//! 4. PreVote - Election optimization to prevent disruption
//! 5. TransferLeadership - Graceful leadership transfer
//! 6. GetClusterInfo - Cluster state and health information
//! 7. AddNode - Dynamic cluster membership addition
//! 8. RemoveNode - Dynamic cluster membership removal

use rustmq::{
    types::*,
    proto::{controller, common},
    proto_convert,
    controller::service::{ControllerService, ControllerRaftServiceImpl},
    config::ScalingConfig,
    error::RustMqError,
};
use std::sync::Arc;
use std::collections::HashMap;
use tokio::sync::RwLock;
use tonic::{Request, Status};
use chrono::Utc;
use prost_types::Timestamp;

// Mock controller for testing Raft operations
pub struct MockControllerRaftService {
    pub node_id: String,
    pub current_term: u64,
    pub voted_for: Option<String>,
    pub log_entries: Vec<controller::LogEntry>,
    pub commit_index: u64,
    pub last_applied: u64,
    pub is_leader: bool,
    pub cluster_nodes: Vec<String>,
    pub should_fail: bool,
    pub failure_code: u32,
}

impl MockControllerRaftService {
    pub fn new(node_id: String) -> Self {
        Self {
            node_id,
            current_term: 0,
            voted_for: None,
            log_entries: Vec::new(),
            commit_index: 0,
            last_applied: 0,
            is_leader: false,
            cluster_nodes: Vec::new(),
            should_fail: false,
            failure_code: 0,
        }
    }

    pub fn with_term(mut self, term: u64) -> Self {
        self.current_term = term;
        self
    }

    pub fn as_leader(mut self) -> Self {
        self.is_leader = true;
        self
    }

    pub fn with_failure(mut self, code: u32) -> Self {
        self.should_fail = true;
        self.failure_code = code;
        self
    }

    pub fn add_log_entry(&mut self, entry_type: controller::LogEntryType, data: Vec<u8>) {
        let entry = controller::LogEntry {
            index: self.log_entries.len() as u64 + 1,
            term: self.current_term,
            r#type: entry_type as i32,
            data,
            timestamp: Some(Timestamp {
                seconds: Utc::now().timestamp(),
                nanos: 0,
            }),
            node_id: self.node_id.clone(),
            checksum: 0,
            data_size: data.len() as u32,
            correlation_id: format!("entry-{}", self.log_entries.len()),
            priority: 0,
            tags: Vec::new(),
        };
        self.log_entries.push(entry);
    }
}

// Create test controller service
fn create_test_controller() -> ControllerService {
    let scaling_config = ScalingConfig {
        max_concurrent_additions: 3,
        max_concurrent_decommissions: 1,
        rebalance_timeout_ms: 300_000,
        traffic_migration_rate: 0.1,
        health_check_timeout_ms: 30_000,
    };

    ControllerService::new(
        "test-controller-1".to_string(),
        vec![],
        scaling_config
    )
}

// Create test Raft service implementation
fn create_test_raft_service() -> ControllerRaftServiceImpl {
    let controller = create_test_controller();
    ControllerRaftServiceImpl::new(Arc::new(RwLock::new(controller)))
}

// Helper function to create test timestamp
fn current_timestamp() -> Timestamp {
    Timestamp {
        seconds: Utc::now().timestamp(),
        nanos: 0,
    }
}

#[tokio::test]
async fn test_request_vote_success() {
    let service = create_test_raft_service();
    
    let proto_request = controller::RequestVoteRequest {
        term: 2,
        candidate_id: "candidate-1".to_string(),
        last_log_index: 5,
        last_log_term: 1,
        pre_vote: false,
        metadata: None,
        candidate_priority: 100,
        candidate_version: "1.0.0".to_string(),
        candidate_capabilities: vec!["replication".to_string(), "scaling".to_string()],
        election_reason: "leader_timeout".to_string(),
        election_timeout_ms: 5000,
    };

    let result = service.request_vote(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // Should grant vote if candidate is up-to-date
    assert!(response.term >= 2);
    assert_eq!(response.voter_id, "test-controller-1");
}

#[tokio::test]
async fn test_request_vote_stale_term() {
    let service = create_test_raft_service();
    
    // First, establish a higher term
    let establish_term_request = controller::RequestVoteRequest {
        term: 5,
        candidate_id: "other-candidate".to_string(),
        last_log_index: 10,
        last_log_term: 3,
        pre_vote: false,
        metadata: None,
        candidate_priority: 100,
        candidate_version: "1.0.0".to_string(),
        candidate_capabilities: vec![],
        election_reason: "leader_timeout".to_string(),
        election_timeout_ms: 5000,
    };
    
    let _ = service.request_vote(Request::new(establish_term_request)).await;
    
    // Now test with stale term
    let stale_request = controller::RequestVoteRequest {
        term: 3, // Lower than previously seen term
        candidate_id: "stale-candidate".to_string(),
        last_log_index: 5,
        last_log_term: 1,
        pre_vote: false,
        metadata: None,
        candidate_priority: 100,
        candidate_version: "1.0.0".to_string(),
        candidate_capabilities: vec![],
        election_reason: "leader_timeout".to_string(),
        election_timeout_ms: 5000,
    };

    let result = service.request_vote(Request::new(stale_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert!(!response.vote_granted); // Should reject stale term
    assert!(response.term >= 5); // Should return current term
}

#[tokio::test]
async fn test_request_vote_validation() {
    let service = create_test_raft_service();
    
    // Test various validation scenarios
    let test_cases = vec![
        (controller::RequestVoteRequest {
            term: 0, // Invalid term
            candidate_id: "candidate-1".to_string(),
            last_log_index: 5,
            last_log_term: 1,
            pre_vote: false,
            metadata: None,
            candidate_priority: 100,
            candidate_version: "1.0.0".to_string(),
            candidate_capabilities: vec![],
            election_reason: "test".to_string(),
            election_timeout_ms: 5000,
        }, "Zero term"),
        (controller::RequestVoteRequest {
            term: 1,
            candidate_id: "".to_string(), // Empty candidate ID
            last_log_index: 5,
            last_log_term: 1,
            pre_vote: false,
            metadata: None,
            candidate_priority: 100,
            candidate_version: "1.0.0".to_string(),
            candidate_capabilities: vec![],
            election_reason: "test".to_string(),
            election_timeout_ms: 5000,
        }, "Empty candidate ID"),
    ];

    for (request, description) in test_cases {
        let result = service.request_vote(Request::new(request)).await;
        // These may pass but should be handled appropriately
        println!("Test case '{}' result: {:?}", description, result.is_ok());
    }
}

#[tokio::test]
async fn test_pre_vote_optimization() {
    let service = create_test_raft_service();
    
    let proto_request = controller::PreVoteRequest {
        term: 2,
        candidate_id: "candidate-1".to_string(),
        last_log_index: 5,
        last_log_term: 1,
        metadata: None,
        election_reason: "leader_timeout".to_string(),
        leader_lease_timeout_ms: 10000,
        last_leader_contact: Some(current_timestamp()),
        candidate_capabilities: vec!["replication".to_string()],
        candidate_priority: 100,
    };

    let result = service.pre_vote(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert_eq!(response.voter_id, "test-controller-1");
}

#[tokio::test]
async fn test_append_entries_heartbeat() {
    let service = create_test_raft_service();
    
    // Test heartbeat (empty entries)
    let proto_request = controller::AppendEntriesRequest {
        term: 1,
        leader_id: "leader-1".to_string(),
        prev_log_index: 0,
        prev_log_term: 0,
        entries: vec![], // Empty for heartbeat
        leader_commit: 0,
        metadata: None,
        is_heartbeat: true,
        batch_size: 0,
        total_batch_size_bytes: 0,
        leader_log_size: 100,
        leader_snapshot_index: 0,
        max_entries_per_request: 1000,
        max_bytes_per_request: 1024 * 1024,
    };

    let result = service.append_entries(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert_eq!(response.follower_id, "test-controller-1");
}

#[tokio::test]
async fn test_append_entries_with_logs() {
    let service = create_test_raft_service();
    
    // Create test log entries
    let log_entry = controller::LogEntry {
        index: 1,
        term: 1,
        r#type: controller::LogEntryType::BrokerMetadata as i32,
        data: b"test-broker-metadata".to_vec(),
        timestamp: Some(current_timestamp()),
        node_id: "leader-1".to_string(),
        checksum: 12345,
        data_size: 20,
        correlation_id: "test-corr-1".to_string(),
        priority: 0,
        tags: vec!["test".to_string()],
    };

    let proto_request = controller::AppendEntriesRequest {
        term: 1,
        leader_id: "leader-1".to_string(),
        prev_log_index: 0,
        prev_log_term: 0,
        entries: vec![log_entry],
        leader_commit: 0,
        metadata: None,
        is_heartbeat: false,
        batch_size: 1,
        total_batch_size_bytes: 20,
        leader_log_size: 100,
        leader_snapshot_index: 0,
        max_entries_per_request: 1000,
        max_bytes_per_request: 1024 * 1024,
    };

    let result = service.append_entries(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert_eq!(response.follower_id, "test-controller-1");
}

#[tokio::test]
async fn test_append_entries_conflict_detection() {
    let service = create_test_raft_service();
    
    // Test log conflict scenario
    let proto_request = controller::AppendEntriesRequest {
        term: 2,
        leader_id: "leader-1".to_string(),
        prev_log_index: 5, // Assuming we don't have this entry
        prev_log_term: 1,
        entries: vec![],
        leader_commit: 0,
        metadata: None,
        is_heartbeat: false,
        batch_size: 0,
        total_batch_size_bytes: 0,
        leader_log_size: 100,
        leader_snapshot_index: 0,
        max_entries_per_request: 1000,
        max_bytes_per_request: 1024 * 1024,
    };

    let result = service.append_entries(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // May indicate conflict if prev_log_index doesn't match
    assert_eq!(response.follower_id, "test-controller-1");
}

#[tokio::test]
async fn test_install_snapshot_success() {
    let service = create_test_raft_service();
    
    let snapshot_data = b"snapshot-data-chunk-1";
    
    let proto_request = controller::InstallSnapshotRequest {
        term: 1,
        leader_id: "leader-1".to_string(),
        last_included_index: 100,
        last_included_term: 1,
        offset: 0,
        data: snapshot_data.to_vec(),
        done: false, // More chunks to come
        metadata: None,
        snapshot_size_bytes: 1024,
        chunk_size_bytes: snapshot_data.len() as u32,
        chunk_index: 0,
        total_chunks: 5,
        snapshot_id: "snap-123".to_string(),
        snapshot_checksum: 54321,
        snapshot_timestamp: Some(current_timestamp()),
        compression: common::CompressionType::None as i32,
        uncompressed_size: snapshot_data.len() as u64,
    };

    let result = service.install_snapshot(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert_eq!(response.follower_id, "test-controller-1");
    assert_eq!(response.bytes_received, snapshot_data.len() as u64);
}

#[tokio::test]
async fn test_install_snapshot_final_chunk() {
    let service = create_test_raft_service();
    
    let snapshot_data = b"final-snapshot-chunk";
    
    let proto_request = controller::InstallSnapshotRequest {
        term: 1,
        leader_id: "leader-1".to_string(),
        last_included_index: 100,
        last_included_term: 1,
        offset: 1000,
        data: snapshot_data.to_vec(),
        done: true, // Final chunk
        metadata: None,
        snapshot_size_bytes: 1024,
        chunk_size_bytes: snapshot_data.len() as u32,
        chunk_index: 4,
        total_chunks: 5,
        snapshot_id: "snap-123".to_string(),
        snapshot_checksum: 54321,
        snapshot_timestamp: Some(current_timestamp()),
        compression: common::CompressionType::Lz4 as i32,
        uncompressed_size: snapshot_data.len() as u64 * 2, // Compressed data
    };

    let result = service.install_snapshot(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert_eq!(response.follower_id, "test-controller-1");
}

#[tokio::test]
async fn test_transfer_leadership_success() {
    let service = create_test_raft_service();
    
    let proto_request = controller::TransferLeadershipRequest {
        current_leader_id: "current-leader".to_string(),
        target_leader_id: "new-leader".to_string(),
        timeout_ms: 30000,
        metadata: None,
        transfer_reason: "planned_maintenance".to_string(),
        force_transfer: false,
    };

    let result = service.transfer_leadership(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // Transfer may succeed or fail depending on current state
    println!("Transfer result: success={}", response.success);
}

#[tokio::test]
async fn test_transfer_leadership_forced() {
    let service = create_test_raft_service();
    
    let proto_request = controller::TransferLeadershipRequest {
        current_leader_id: "current-leader".to_string(),
        target_leader_id: "new-leader".to_string(),
        timeout_ms: 5000, // Short timeout
        metadata: None,
        transfer_reason: "emergency".to_string(),
        force_transfer: true, // Force transfer
    };

    let result = service.transfer_leadership(Request::new(proto_request)).await;
    assert!(result.is_ok());
}

#[tokio::test]
async fn test_get_cluster_info_basic() {
    let service = create_test_raft_service();
    
    let proto_request = controller::GetClusterInfoRequest {
        metadata: None,
        include_node_details: false,
        include_log_info: false,
        include_performance_metrics: false,
    };

    let result = service.get_cluster_info(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert!(!response.cluster_id.is_empty());
}

#[tokio::test]
async fn test_get_cluster_info_detailed() {
    let service = create_test_raft_service();
    
    let proto_request = controller::GetClusterInfoRequest {
        metadata: None,
        include_node_details: true,
        include_log_info: true,
        include_performance_metrics: true,
    };

    let result = service.get_cluster_info(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    assert!(!response.cluster_id.is_empty());
    // Should include detailed information
    assert!(response.current_term >= 0);
    assert!(response.commit_index >= 0);
}

#[tokio::test]
async fn test_add_node_success() {
    let service = create_test_raft_service();
    
    let proto_request = controller::AddNodeRequest {
        node_id: "new-node-1".to_string(),
        node_address: "192.168.1.100:9090".to_string(),
        voting_member: true,
        metadata: None,
        capabilities: vec!["replication".to_string(), "consensus".to_string()],
        priority: 100,
        version: "1.0.0".to_string(),
    };

    let result = service.add_node(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // May succeed or fail depending on current leadership state
    println!("Add node result: success={}", response.success);
}

#[tokio::test]
async fn test_add_node_validation() {
    let service = create_test_raft_service();
    
    // Test adding node with invalid address
    let proto_request = controller::AddNodeRequest {
        node_id: "new-node-1".to_string(),
        node_address: "invalid-address".to_string(), // Invalid format
        voting_member: true,
        metadata: None,
        capabilities: vec![],
        priority: 100,
        version: "1.0.0".to_string(),
    };

    let result = service.add_node(Request::new(proto_request)).await;
    assert!(result.is_ok()); // Should handle gracefully
}

#[tokio::test]
async fn test_add_node_non_voting() {
    let service = create_test_raft_service();
    
    // Test adding non-voting observer node
    let proto_request = controller::AddNodeRequest {
        node_id: "observer-node-1".to_string(),
        node_address: "192.168.1.101:9090".to_string(),
        voting_member: false, // Non-voting observer
        metadata: None,
        capabilities: vec!["monitoring".to_string()],
        priority: 50, // Lower priority
        version: "1.0.0".to_string(),
    };

    let result = service.add_node(Request::new(proto_request)).await;
    assert!(result.is_ok());
}

#[tokio::test]
async fn test_remove_node_success() {
    let service = create_test_raft_service();
    
    let proto_request = controller::RemoveNodeRequest {
        node_id: "node-to-remove".to_string(),
        force_removal: false,
        metadata: None,
        removal_reason: "planned_decommission".to_string(),
    };

    let result = service.remove_node(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    println!("Remove node result: success={}", response.success);
}

#[tokio::test]
async fn test_remove_node_forced() {
    let service = create_test_raft_service();
    
    let proto_request = controller::RemoveNodeRequest {
        node_id: "failed-node".to_string(),
        force_removal: true, // Force removal of failed node
        metadata: None,
        removal_reason: "node_failure".to_string(),
    };

    let result = service.remove_node(Request::new(proto_request)).await;
    assert!(result.is_ok());
}

#[tokio::test]
async fn test_log_entry_types() {
    let service = create_test_raft_service();
    
    // Test different log entry types
    let entry_types = vec![
        controller::LogEntryType::NoOp,
        controller::LogEntryType::Configuration,
        controller::LogEntryType::BrokerMetadata,
        controller::LogEntryType::TopicMetadata,
        controller::LogEntryType::PartitionAssignment,
        controller::LogEntryType::LeadershipChange,
        controller::LogEntryType::ScalingOperation,
        controller::LogEntryType::UpgradeOperation,
        controller::LogEntryType::AdminOperation,
        controller::LogEntryType::Heartbeat,
    ];

    for entry_type in entry_types {
        let log_entry = controller::LogEntry {
            index: 1,
            term: 1,
            r#type: entry_type as i32,
            data: format!("{:?}-data", entry_type).into_bytes(),
            timestamp: Some(current_timestamp()),
            node_id: "test-node".to_string(),
            checksum: 12345,
            data_size: 10,
            correlation_id: format!("corr-{:?}", entry_type),
            priority: 0,
            tags: vec![format!("{:?}", entry_type)],
        };

        let proto_request = controller::AppendEntriesRequest {
            term: 1,
            leader_id: "leader-1".to_string(),
            prev_log_index: 0,
            prev_log_term: 0,
            entries: vec![log_entry],
            leader_commit: 0,
            metadata: None,
            is_heartbeat: false,
            batch_size: 1,
            total_batch_size_bytes: 10,
            leader_log_size: 100,
            leader_snapshot_index: 0,
            max_entries_per_request: 1000,
            max_bytes_per_request: 1024 * 1024,
        };

        let result = service.append_entries(Request::new(proto_request)).await;
        assert!(result.is_ok(), "Failed for entry type {:?}", entry_type);
    }
}

#[tokio::test]
async fn test_concurrent_vote_requests() {
    let service = Arc::new(create_test_raft_service());
    let mut handles = Vec::new();

    // Send multiple concurrent vote requests
    for i in 0..5 {
        let service_clone = service.clone();
        let handle = tokio::spawn(async move {
            let proto_request = controller::RequestVoteRequest {
                term: i + 1,
                candidate_id: format!("candidate-{}", i),
                last_log_index: 5,
                last_log_term: 1,
                pre_vote: false,
                metadata: None,
                candidate_priority: 100,
                candidate_version: "1.0.0".to_string(),
                candidate_capabilities: vec![],
                election_reason: "concurrent_test".to_string(),
                election_timeout_ms: 5000,
            };

            service_clone.request_vote(Request::new(proto_request)).await
        });
        handles.push(handle);
    }

    // All should complete without panic
    for handle in handles {
        let result = handle.await.unwrap();
        assert!(result.is_ok());
    }
}

#[tokio::test]
async fn test_large_log_batch() {
    let service = create_test_raft_service();
    
    // Create large batch of log entries
    let mut entries = Vec::new();
    for i in 0..100 {
        let entry = controller::LogEntry {
            index: i + 1,
            term: 1,
            r#type: controller::LogEntryType::BrokerMetadata as i32,
            data: format!("broker-metadata-{}", i).into_bytes(),
            timestamp: Some(current_timestamp()),
            node_id: "leader-1".to_string(),
            checksum: 12345 + i,
            data_size: 20,
            correlation_id: format!("corr-{}", i),
            priority: 0,
            tags: vec!["batch-test".to_string()],
        };
        entries.push(entry);
    }

    let proto_request = controller::AppendEntriesRequest {
        term: 1,
        leader_id: "leader-1".to_string(),
        prev_log_index: 0,
        prev_log_term: 0,
        entries,
        leader_commit: 0,
        metadata: None,
        is_heartbeat: false,
        batch_size: 100,
        total_batch_size_bytes: 2000,
        leader_log_size: 200,
        leader_snapshot_index: 0,
        max_entries_per_request: 1000,
        max_bytes_per_request: 1024 * 1024,
    };

    let result = service.append_entries(Request::new(proto_request)).await;
    assert!(result.is_ok());
}

#[tokio::test]
async fn test_metadata_propagation() {
    let service = create_test_raft_service();
    
    let metadata = common::RequestMetadata {
        request_id: "raft-test-123".to_string(),
        timestamp: Some(current_timestamp()),
        client_id: "test-client".to_string(),
        timeout_ms: 30000,
        retry_count: 0,
        correlation_id: "raft-corr-123".to_string(),
        source_broker_id: "source".to_string(),
        target_broker_id: "target".to_string(),
        operation_type: "consensus".to_string(),
        priority: 1,
        trace_context: HashMap::new(),
    };

    let proto_request = controller::RequestVoteRequest {
        term: 1,
        candidate_id: "candidate-1".to_string(),
        last_log_index: 5,
        last_log_term: 1,
        pre_vote: false,
        metadata: Some(metadata),
        candidate_priority: 100,
        candidate_version: "1.0.0".to_string(),
        candidate_capabilities: vec![],
        election_reason: "metadata_test".to_string(),
        election_timeout_ms: 5000,
    };

    let result = service.request_vote(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // Metadata should be processed correctly
    assert!(response.metadata.is_some());
}

#[tokio::test]
async fn test_node_state_transitions() {
    let service = create_test_raft_service();
    
    // Test various node states in cluster info
    let proto_request = controller::GetClusterInfoRequest {
        metadata: None,
        include_node_details: true,
        include_log_info: false,
        include_performance_metrics: false,
    };

    let result = service.get_cluster_info(Request::new(proto_request)).await;
    assert!(result.is_ok());
    
    let response = result.unwrap().into_inner();
    // Check that we can handle different node states
    for node in response.nodes {
        match controller::NodeState::try_from(node.state) {
            Ok(state) => {
                println!("Node {} in state {:?}", node.node_id, state);
                assert!(!node.node_id.is_empty());
            }
            Err(_) => {
                // Handle unknown state gracefully
                println!("Unknown node state: {}", node.state);
            }
        }
    }
}